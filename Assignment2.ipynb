{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CS3263 - Assignment 2\n",
    "---\n",
    "#### Sem 2, AY 2024/25\n",
    "\n",
    "👋 Welcome to the Assignment 2 of course CS3263 - Foundations of Artificial Intelligence! \n",
    "\n",
    "# Guideline\n",
    "\n",
    "- Please complete this Assignment in a **TWO-person team**.\n",
    "- **ONE person from each team** will need to submit **ALL** the answers.\n",
    "- Grading will be done team-wise. Please register your groups on Canvas.\n",
    "- The deadline for the assignment is **`7 April 2025, 2359`**.\n",
    "\n",
    "You are encouraged to discuss solution ideas. However, each team *must write up the solutions independently*. It is considered plagiarism if the solution write-up is highly similar to other teams' write-ups or other sources.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Team Info\n",
    "\n",
    "- **Group Member 1:**\n",
    "    * Name:\n",
    "    * Student ID:\n",
    "\n",
    "- **Group Member 2:**\n",
    "    * Name:\n",
    "    * Student ID:\n",
    "\n",
    "---\n",
    "\n",
    "# Submission\n",
    "\n",
    "- You can download the programming package from Canvas or from https://github.com/CS3263-AI-Sem2-2425/Assignment2.\n",
    "- No late submissions are allowed.\n",
    "- Complete the codes in `Assignment2.ipynb` and **copy** your solutions to `programming.py`.\n",
    "- Please write the following on the top of your solution files:\n",
    "    - The **name(s)** and **metric number(s)** of ALL team members as they appear on Canvas.\n",
    "    - Collaborators (write *None* if no collaborators).\n",
    "    - Sources, if any, or if you obtained the solutions through research, e.g., through the web.\n",
    "- Your programming part should be zipped and named `netid1-netid2.zip`. **Only submit the zipped file to Canvas**.\n",
    "- **IMPORTANT:** The programming part will be auto-graded.\n",
    "    - Note that the `programming.py` file is designed for auto-grading, so you cannot run it normally from your end but don't worry.\n",
    "    - DO NOT modify any code outside the blocks you're allowed to; DO NOT print any message in `programming.py`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment\n",
    "In this programming assignment, we are going to solve the searching problems in a gird maze environment.\n",
    "\n",
    "This notebook will walk you through how to use the environment, how to define your own agent, and how to solve the problems.\n",
    "\n",
    "You are expected to add your codes in the blocks noted by:\n",
    "```python\n",
    "# ------- your code starts here ----- #\n",
    "\n",
    "# Some code examples ...\n",
    "\n",
    "# ------- your code ends here ------- #\n",
    "```\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Environment Setup\n",
    "To set up the Python programming environment, three basic packages, i.e., `typer`, `typing`, and `numpy` are needed.\n",
    "\n",
    "You can install them by the way you like.\n",
    "\n",
    "- For example, install using `pip`:\n",
    "\n",
    "    ```\n",
    "    pip install typer numpy\n",
    "    ```\n",
    "\n",
    "- Or install using `conda`:\n",
    "\n",
    "    ```\n",
    "    conda install typer numpy\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Callable\n",
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousemodel import state_class, item_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minihouse Environment\n",
    "All environments are defined in the `minihouse` package.\n",
    "\n",
    "Each of the **states** and **actions** are represented as an integer.\n",
    "\n",
    "You can use the following functions to convert between the integer and the corresponding state description.\n",
    "\n",
    "```python\n",
    "goal_state = state_class(\n",
    "    robot_agent=None,\n",
    "    human_agent=None,\n",
    "    object_list={\"apple\": item_object(\"apple\", True, \"table\")},\n",
    "    container_list=None,\n",
    "    surface_list=None,\n",
    "    room_list=None,\n",
    ")\n",
    "\n",
    "env = MiniHouseV1(\n",
    "    instruction=\"move the apple to the table\",\n",
    "    goal_state=goal_state,\n",
    ")\n",
    "env.reset()\n",
    "print(env.state)\n",
    "print(env.state_to_index(env.state))\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniHouseV1 Environment Instructions:\n",
    "```python\n",
    "env.nS # Number of states\n",
    "env.nA # Number of states\n",
    "env.state.room_dict env.state.container_dict env.state.surface_dict env.state.item_dict # get object info\n",
    "env.state.container_dict['name'].position # get object position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Policy Iteration & Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you are expected to implement the **policy iteration** and **value iteration** algorithms to solve the searching problems in the `MiniHouse` environment. \n",
    "\n",
    "Here, we provide a template for you to implement your own agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdp_solver:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.iteration = 0\n",
    "        print(\"MDP initialized!\")\n",
    "        \n",
    "    \n",
    "    def get_action_value(\n",
    "        self, s:int, a:int, V:np.ndarray, gamma:float, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        Code for getting action value. Compute the value of taking action a in state s\n",
    "        I.e., compute Q(s, a) = sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "        args:\n",
    "            s: state\n",
    "            a: action\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            value: action value\n",
    "        \"\"\"\n",
    "        value = 0\n",
    "        \n",
    "        for prob, next_state, reward, done in env_transition(s, a):\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    \n",
    "    def get_max_action_value(\n",
    "        self, s:int, env_nA:int, env_transition:Callable, V:np.ndarray, gamma:float):\n",
    "        \"\"\"\n",
    "        Code for getting max action value. Takes in the current state and returns \n",
    "        the max action value and action that leads to it. I.e., compute\n",
    "        a* = argmax_a sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "        args:\n",
    "            s: state\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "        returns:\n",
    "            max_value: max action value\n",
    "            max_action: action that leads to max action value\n",
    "        \"\"\"\n",
    "        max_value = -np.inf\n",
    "        max_action = -1\n",
    "        \n",
    "        for a in range(env_nA):\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return max_value, max_action\n",
    "    \n",
    "    \n",
    "    def get_policy(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, gamma:float, V:np.ndarray):\n",
    "        \"\"\"\n",
    "        Code for getting policy. Takes in an Value function and returns the optimal policy\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            gamma: discount factor\n",
    "            V: value function\n",
    "        returns:\n",
    "            policy: policy\n",
    "        \"\"\"\n",
    "        policy = np.zeros(env_nS)\n",
    "        \n",
    "        for s in range(env_nS):\n",
    "            max_value = -np.inf\n",
    "            max_action = -1\n",
    "            for a in range(env_nA):\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "              \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "                    \n",
    "            policy[s] = max_action\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    \n",
    "    def policy_evaluation(\n",
    "        self, env_nS:int, env_transition:Callable, V:np.ndarray, gamma:float, theta:float, policy:np.ndarray):\n",
    "        \"\"\"\n",
    "        Code for policy evaluation. Takes in an MDP and returns the converged value function\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_transition: transition function\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            policy: policy\n",
    "        returns:\n",
    "            V: value function\n",
    "        \"\"\" \n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env_nS):\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "\n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "                \n",
    "            if delta < theta:\n",
    "                break\n",
    "                \n",
    "        return V\n",
    "    \n",
    "    \n",
    "    def policy_improvement(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, policy:np.ndarray, V:np.ndarray, gamma:float):\n",
    "        \"\"\"\n",
    "        Code for policy improvement. Takes in an MDP and returns the converged policy\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            policy: policy\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "        returns:\n",
    "            policy_stable: whether policy is stable\n",
    "            policy: policy\n",
    "        \"\"\"\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(env_nS):\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return policy_stable, policy\n",
    "    \n",
    "    \n",
    "    def value_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        The code for value iteration. Takes in an MDP and returns the optimal policy\n",
    "        and value function.\n",
    "        args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "            V: optimal value function \n",
    "        \"\"\"\n",
    "        V = np.zeros(env_nS)\n",
    "        converged = False\n",
    "        \n",
    "        while not converged:\n",
    "            delta = 0\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        policy = self.get_policy(env_nS, env_nA, env_transition, gamma, V)\n",
    "        return policy, V\n",
    "    \n",
    "    \n",
    "    def policy_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        Code for policy iteration. Takes in an MDP and returns the optimal policy\n",
    "        args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "            V: optimal value function\n",
    "        \"\"\"\n",
    "        V = np.zeros(env_nS)\n",
    "        policy = np.zeros(env_nS)\n",
    "        converged = False\n",
    "        \n",
    "        while not converged:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "            \n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Guideline\n",
    "\n",
    "Here, we introduce the role of each function in the above solution template. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Action Value\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def get_action_value(\n",
    "        self, s:int, a:int, V:np.ndarray, gamma:float, env_transition:Callable):\n",
    "```\n",
    "This function is used to get the action value. The input arguments are:\n",
    "* `s`: state\n",
    "* `a`: action\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "\n",
    "The output arguments are:\n",
    "* `value`: action value, which is a float number.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Action Value\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def get_max_action_value(\n",
    "        self, s:int, env_nA:int, env_transition:Callable, V:np.ndarray, gamma:float):\n",
    "\n",
    "```\n",
    "This function is used to get the max action value. The input arguments are:\n",
    "* `s`: state\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "\n",
    "The output arguments are:\n",
    "* `max_value`: max action value, which is a float number.\n",
    "* `max_action`: action that leads to max action value, which is an integer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Policy\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def get_policy(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, gamma:float, V:np.ndarray):\n",
    "```\n",
    "This function is used to extract a policy given a value function. The input arguments are:\n",
    "* `env_nS`: number of states\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `gamma`: discount factor\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "The output arguments are:\n",
    "* `policy`: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def policy_evaluation(\n",
    "        self, env_nS:int, env_transition:Callable, V:np.ndarray, gamma:float, theta:float, policy:np.ndarray):\n",
    "```\n",
    "This function is used to implement policy iteration. You are expected to implement the policy evaluation algorithm in this function. The input arguments are:\n",
    "* `env_nS`: number of states\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "* `theta`: convergence threshold\n",
    "* `policy`: policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "The output arguments are:\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def policy_improvement(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, policy:np.ndarray, V:np.ndarray, gamma:float):\n",
    "```\n",
    "This function is used to implement policy iteration. You are expected to implement the policy improvement algorithm in this function. The input arguments are:\n",
    "* `env_nS`: number of states\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `policy`: policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "\n",
    "The output arguments are:\n",
    "* `olicy_stable`: whether policy is stable, which is a boolean value.\n",
    "* `policy`: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def value_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "```\n",
    "This function is used to implement value iteration. You are expected to implement the value iteration algorithm in this function. The input arguments are:\n",
    "* `gamma`: discount factor\n",
    "* `theta`: convergence threshold\n",
    "* `env_nS`: number of states\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "\n",
    "The output arguments are:\n",
    "* `policy`: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "* `V`: optimal value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def policy_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    " ```\n",
    " \n",
    "This function is utilized to conduct policy iteration. Through policy iteration, you iteratively improve the policy until it converges to the optimal policy. The process involves two main steps: policy evaluation, where you calculate the value function for a given policy, and policy improvement, where you generate a new policy based on the calculated value function.\n",
    "\n",
    "The input arguments for this function are:\n",
    "\n",
    "- `gamma`: The discount factor, a float that determines the importance of future rewards.\n",
    "- `theta`: The convergence threshold, a small float value that determines when to stop iterating because the value function has sufficiently converged.\n",
    "- `env_nS`: The number of states in the environment, an integer.\n",
    "- `env_nA`: The number of actions available in the environment, an integer.\n",
    "- `env_transition`: The transition function of the environment. It takes a state and an action as input and returns a list of tuples (probability, next_state_index, reward, done). Each tuple represents a possible outcome of taking the action in the given state, where:\n",
    "    - `probability` is a float representing the likelihood of the outcome.\n",
    "    - `next_state_index` is an integer index of the next state.\n",
    "    - `reward` is a float representing the received reward.\n",
    "    - `done` is a boolean indicating whether the episode has ended.\n",
    "\n",
    "The output arguments are:\n",
    "\n",
    "- `policy`: The optimal policy derived from policy iteration. It is a NumPy array where each element represents the best action (as an integer index) to take in the corresponding state.\n",
    "- `V`: The optimal value function, a NumPy array of floats, where each element represents the value of being in the corresponding state under the optimal policy.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try On Your Own!\n",
    "\n",
    "You can try the given test environment to test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousev1 import test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(VI_bool = True, index = 0, verbose=False):\n",
    "\n",
    "    instruction, goal_state, initial_conditions = test_cases[index]\n",
    "\n",
    "    env = MiniHouseV1(\n",
    "        instruction=instruction,\n",
    "        goal_state=goal_state,\n",
    "        initial_conditions=initial_conditions,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    env.reset()\n",
    "    if verbose:\n",
    "        print(\"state: \", env.state_to_index(env.state))\n",
    "        print(\"num state: \", env.nS)\n",
    "        print(\"num actions: \", env.nA)\n",
    "        print()\n",
    "\n",
    "    ms = mdp_solver()\n",
    "    \n",
    "    if VI_bool:\n",
    "        policy, V = ms.value_iteration(0.9, 0.0001, env_nS=env.nS, env_nA=env.nA, env_transition=env.transition)\n",
    "        if verbose:\n",
    "            print(\"Value Iteration\")\n",
    "            print()\n",
    "    else:\n",
    "        policy, V = ms.policy_iteration(0.9, 0.0001, env_nS=env.nS, env_nA=env.nA, env_transition=env.transition)\n",
    "        if verbose:\n",
    "            print(\"Policy Iteration\")\n",
    "            print()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"V: \", repr(V))\n",
    "        print()\n",
    "        print(\"policy: \", repr(policy))\n",
    "        print()\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    for i in range(100):\n",
    "        print()\n",
    "        print(f\"---------- Step: {i} ----------\")\n",
    "        \n",
    "        action = int(policy[env.state_to_index(env.state)])\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"obs: \", obs)\n",
    "            print(\"reward: \", reward)\n",
    "            print(\"done: \", done)\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Value Iteration\n",
    "Run the following code to test your implementation of value iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  9\n",
      "num state:  56\n",
      "num actions:  13\n",
      "\n",
      "MDP initialized!\n",
      "Value Iteration\n",
      "\n",
      "V:  array([499.99959766, 499.99963427, 499.99963427, 499.99966755,\n",
      "       499.99962964, 499.99966334, 499.99966334, 499.99969397,\n",
      "       306.62502756, 345.7146245 , 271.83106078, 240.86060754,\n",
      "       345.7146245 , 389.63008967, 306.62509833, 271.83112512,\n",
      "       438.96711366, 389.6300606 , 389.6300606 , 345.71466242,\n",
      "       438.96714564, 389.63008967, 389.63008967, 345.71468884,\n",
      "       345.71459253, 389.6300606 , 306.62506926, 271.8310987 ,\n",
      "       345.7146245 , 389.63008967, 306.62509833, 271.83112512,\n",
      "       345.71459253, 306.62506926, 389.6300606 , 345.71466242,\n",
      "       345.7146245 , 306.62509833, 389.63008967, 345.71468884,\n",
      "       271.83102881, 240.86057848, 306.62506926, 345.71466242,\n",
      "       271.83106078, 240.86060754, 306.62509833, 345.71468884,\n",
      "       494.39523867, 438.96718355, 438.96718355, 389.63012413,\n",
      "       494.39526773, 438.96720997, 438.96720997, 389.63014815])\n",
      "\n",
      "policy:  array([1., 4., 3., 0., 1., 1., 3., 0., 1., 4., 0., 2., 1., 6., 0., 2., 6.,\n",
      "       0., 0., 2., 6., 0., 0., 2., 1., 6., 0., 2., 1., 6., 0., 2., 2., 0.,\n",
      "       6., 2., 2., 0., 6., 2., 2., 0., 3., 6., 2., 0., 3., 6., 8., 0., 0.,\n",
      "       2., 8., 0., 0., 2.])\n",
      "\n",
      "\n",
      "---------- Step: 0 ----------\n",
      "action:  open the fridge\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is at the fridge. \n",
      "\n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 1 ----------\n",
      "action:  pick the apple\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 2 ----------\n",
      "action:  move to the living room\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 3 ----------\n",
      "action:  move to the living room\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 4 ----------\n",
      "action:  place the apple at the table\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. \n",
      "reward:  50\n",
      "done:  True\n"
     ]
    }
   ],
   "source": [
    "test(index=0, VI_bool=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Policy Iteration\n",
    "\n",
    "Run the following code to test your implementation of policy iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(index=0, VI_bool=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have finished all the requirements for Problem 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you are expected to implement the **Q-learning** algorithm to solve the searching problems in the `MiniHouse` environment. \n",
    "\n",
    "Similar to the previous problem, we provide a template for you to implement your own agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdp_solver_q_learning:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"MDP initialized for Q-learning!\")\n",
    "\n",
    "\n",
    "    def epsilon_greedy(self, Q, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "\n",
    "            \n",
    "            # ------- your code ends here ------- #\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "\n",
    "        \n",
    "            # ------- your code ends here ------- #\n",
    "   \n",
    "\n",
    "    def Q_learning(\n",
    "        self, alpha:float, gamma:float, theta:float, epsilon:float, env_nS:int, env_nA:int, env_transition, env, num_episodes=1000, initial_action = None, initial_reward = 10):\n",
    "        \"\"\"\n",
    "        Q-learning algorithm.\n",
    "        Args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            num_episodes: number of episodes\n",
    "        Returns:\n",
    "            Q: learned Q-value function\n",
    "            rewards: rewards obtained in each episode\n",
    "        \"\"\"\n",
    "        Q = np.zeros((env_nS, env_nA))\n",
    "        rewards = []\n",
    "        \n",
    "        if initial_action is not None:\n",
    "            for i, action in enumerate(initial_action):\n",
    "                Q[i][action] += initial_reward\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            env.reset()\n",
    "            state = env.state_to_index(env.state)\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "            \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "        \n",
    "        return np.argmax(Q, axis=1), rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Guideline\n",
    "\n",
    "Here, we introduce the role of each function in the above solution template. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Action Selection\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def epsilon_greedy(self, Q, state, epsilon):\n",
    "```\n",
    "\n",
    "This function selects an action based on the epsilon-greedy strategy, which balances exploration and exploitation. With a probability of epsilon, it chooses a random action (exploration), and with a probability of 1 - epsilon, it chooses the action with the highest Q-value for the current state (exploitation).\n",
    "\n",
    "The input arguments are:\n",
    "\n",
    "- `Q`: The Q-value table, a 2D NumPy array where each element Q[state, action] represents the estimated value of taking an action in a state.\n",
    "- `state`: The current state index, an integer representing the current situation of the environment.\n",
    "- `epsilon`: The exploration rate, a float between 0 and 1 that determines the probability of choosing a random action.\n",
    "\n",
    "The output is:\n",
    "- An integer representing the index of the selected action.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "def Q_learning(\n",
    "        self, alpha:float, gamma:float, theta:float, epsilon:float, env_nS:int, env_nA:int, env_transition, env, num_episodes=1000, initial_action = None, initial_reward = 10):\n",
    "```\n",
    "\n",
    "This function implements the Q-learning algorithm, a model-free reinforcement learning technique used to learn the quality of actions denoting how good it is to take an action from a particular state.\n",
    "\n",
    "The input arguments are:\n",
    "- `alpha`: The learning rate, a float that determines the extent to which the newly acquired information will override the old information.\n",
    "- `gamma`: The discount factor, a float that balances the importance of immediate and future rewards.\n",
    "- `theta`: The convergence threshold, not directly used in basic Q-learning but can be utilized for extensions or stopping criteria based on improvements.\n",
    "- `epsilon`: Exploration rate in the epsilon-greedy strategy, determining the trade-off between exploration and exploitation.\n",
    "- `env_nS`: The number of states in the environment, indicating how many distinct states the agent can be in.\n",
    "- `env_nA`: The number of actions available in the environment, indicating the range of actions the agent can take.\n",
    "- `env_transition`: The transition function of the environment that models the dynamics of the environment. It's used to simulate the next state and reward given a state-action pair.\n",
    "- `env`: The environment object itself, providing access to essential methods like reset and state_to_index.\n",
    "- `num_episodes`: The number of episodes to run the algorithm for, allowing the agent sufficient time to learn from its interactions with the environment.\n",
    "- `initial_action`: Initial action for each state.\n",
    "- `initial_reward`: Initial reward for given initial state.\n",
    "\n",
    "The outputs are:\n",
    "- `Q`: The learned Q-value function, represented as a 2D numpy array where each element [s, a] corresponds to the estimated value of taking action a in state s.\n",
    "- `rewards`: A list of accumulated rewards, one for each episode, indicating the total reward the agent has obtained during that episode.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try On Your Own!\n",
    "\n",
    "You can try the given test environment to test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousev1 import test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Q-Learning\n",
    "\n",
    "Run the following code to test your implementation of the Q-learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_learning(Q_bool=False, index=0, num_episodes=1000, verbose=False, generate_solution=False):\n",
    "\n",
    "    instruction, goal_state, initial_conditions = test_cases[index]\n",
    "\n",
    "    env = MiniHouseV1(\n",
    "        instruction=instruction,\n",
    "        goal_state=goal_state,\n",
    "        initial_conditions=initial_conditions,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    env.reset()\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"state: \", env.state_to_index(env.state))\n",
    "        print(\"num state: \", env.nS)\n",
    "        print(\"num actions: \", env.nA)\n",
    "        print()\n",
    "        \n",
    "    msq = mdp_solver_q_learning()\n",
    "\n",
    "\n",
    "    policy, V = msq.Q_learning(\n",
    "        alpha=0.1,\n",
    "        gamma=0.9,\n",
    "        theta=0.0001,\n",
    "        epsilon=0.1,\n",
    "        env_nS=env.nS,\n",
    "        env_nA=env.nA,\n",
    "        env_transition=env.transition,\n",
    "        env=env,\n",
    "        num_episodes=num_episodes,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Policy Iteration\")\n",
    "        print()\n",
    "        print(\"V: \", repr(V))\n",
    "        print()\n",
    "        print(\"policy: \", repr(policy))\n",
    "        print()\n",
    "        \n",
    "    env.reset()\n",
    "    \n",
    "    for i in range(100):\n",
    "        print()\n",
    "        print(f\"---------- Step: {i} ----------\")\n",
    "        action = int(policy[env.state_to_index(env.state)])\n",
    "        # action = policy[env.state_to_index(env.state)].astype(int)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        if verbose:\n",
    "            print(\"obs: \", obs)\n",
    "            print(\"reward: \", reward)\n",
    "            print(\"done: \", done)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if generate_solution:\n",
    "        np.savetxt(f'data/V_{index}.py', V, delimiter=',')\n",
    "\n",
    "        solution_values = np.loadtxt(f'data/V_{index}.py')\n",
    "\n",
    "        assert len(V) == len(solution_values), \\\n",
    "            'Length of Values is incorrect'\n",
    "\n",
    "        assert np.allclose(V, solution_values), \\\n",
    "            'Values incorrect'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q_learning(index=0, num_episodes=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have finished all the requirements for Problem 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Integrating GPT-4 for Decision Support in MiniHouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this problem, you are expected to enhance the **Q-learning** algorithm implemented in problem 2 by integrating Generative Models for decision support in the MiniHouse environment\n",
    "\n",
    "Tasks to Accomplish:\n",
    "- **Prompt Design:** Crafting detailed and clear prompts that accurately describe the decision-making problem and obtaining useful suggestions from Generative Models.\n",
    "- **API Key:** Ensure you have a valid API key from the respective provider of the Generative Model you're using.\n",
    "- **Post-processing:** The action generated by GPT-4 might require post-processing or validation to ensure they fit within the constraints and capabilities of the MiniHouse simulation.\n",
    "- **Integration:** This approach can serve as a high-level heuristic or guide for developing or refining decision-making strategies within your environment. Actual integration might involve translating natural language actions into environment-specific operations or commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we recommand using LLaMA as the primary Large Language Model due to its strong generation ability, contextual understanding, and easy to access(open-source). LLaMA's high-quality text generation makes it well-suited for assisting in reinforcement learning and search-based problems. However, students are encouraged to explore other LLMs such as GPT4, Claude, Gemini or Deepseek. \n",
    "\n",
    "When choosing your own LLM, you should consider factors like efficiency, API availability, fine-tuning capabilities, and prompt adherence to ensure the model aligns well with the task requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Guideline\n",
    "The goal of this problem is to utilize generative model as a heuristics for Q-learning algorithm. Here, we introduce the role of each function in the above solution template. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to use the generative model API, you need to install OpenAI Python package by \n",
    "\n",
    "```python\n",
    "pip install openai\n",
    "pip install pytorch (please check the correct version through pytorch website)\n",
    "pip install sentence-transformers\n",
    "conda install -c conda-forge openai\n",
    "conda install -c pytorch pytorch\n",
    "conda install -c conda-forge sentence-transformers\n",
    "```\n",
    "\n",
    "You need to create a client for Generative model API. You can sign up on openai/nvidia/other website for API key generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# ------- your code starts here ----- #\n",
    "client = OpenAI(\n",
    "    base_url = \"\",\n",
    "    api_key = \"Your api key\"\n",
    ")\n",
    "# ------- your code ends here ------- #\n",
    "    \n",
    "\n",
    "# create a client to call the generative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## describe_state\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "def describe_state(state):\n",
    "```\n",
    "\n",
    "\n",
    "This function implements the natural language describtion generation of the current state.\n",
    "\n",
    "The input arguments are:\n",
    "- `state`: The current state of the Minihouse environment.\n",
    "\n",
    "The outputs are:\n",
    "- `describe`: Natural language discribtion of the current state. This description will be included in the prompt fed into the generative model.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe your current state\n",
    "def describe_state(state):\n",
    "    describe = ''\n",
    "\n",
    "    # ------- your code starts here ----- #\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------- your code ends here ------- #\n",
    "    \n",
    "    \n",
    "    print(describe)\n",
    "    return describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan\n",
    "\n",
    "LLM_Model class:\n",
    "\n",
    "```python\n",
    "def plan(self, task, observe):\n",
    "```\n",
    "\n",
    "\n",
    "This function implements the next move planning task using LLM.\n",
    "\n",
    "The input arguments are:\n",
    "- `task`: The final goal of the planning task.\n",
    "- `observe`: The natural language describtion of the current state.\n",
    "\n",
    "The outputs are:\n",
    "- `best_action`: The optimal action generatated from LLM.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "The ideal output should be like:\n",
    "\n",
    "'open fridge', 'close fridge', 'pick apple', 'place apple fridge', 'place apple table', 'place apple bedroom', 'place apple bathroom', 'place apple living room', 'move living room', 'move kitchen', 'move bedroom', 'move bathroom', etc.\n",
    "\n",
    "But since different models may interpret prompts differently and generate varying outputs, you should experiment with prompt engineering to achieve reliable and relevant results for the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_most_similar\n",
    "\n",
    "\n",
    "```python\n",
    "def find_most_similar(self, query_str, corpus_embedding):\n",
    "```\n",
    "\n",
    "\n",
    "This function aims to find the most similar valid action given the action generated from LLM.\n",
    "\n",
    "The input arguments are:\n",
    "- `query_str`: The action generated from LLM.\n",
    "- `corpus_embedding`: The embedding of the valid action in action list.\n",
    "\n",
    "The outputs are:\n",
    "- `most_similar_idx`: The index of the most similar valid action.\n",
    "\n",
    "\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query_llm\n",
    "\n",
    "```python\n",
    "def query_llm(self, task, observe = None):\n",
    "```\n",
    "\n",
    "Query LLM to get the next move.\n",
    "\n",
    "The input arguments are:\n",
    "- `task`: The final goal of the planning task.\n",
    "- `observe`: The natural language describtion of the current state.\n",
    "\n",
    "The outputs are:\n",
    "- `samples`: Samples generated from n times LLM query.\n",
    "\n",
    "\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as st_utils\n",
    "\n",
    "class LLM_Model:\n",
    "    def __init__(self, device, instruction, goal_state, initial_conditions, model='gpt-3.5-turbo'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.sampling_params = \\\n",
    "            {\n",
    "                \"max_tokens\": 32,\n",
    "                \"temperature\": 0.5,\n",
    "                \"top_p\": 0.9,\n",
    "                \"n\": 1,\n",
    "                \"presence_penalty\": 0.5,\n",
    "                \"frequency_penalty\": 0.3,\n",
    "                \"stop\": ['\\n']\n",
    "            }\n",
    "        \n",
    "        self.prompt_begin = \"\"\"Generate the most logical next move in the scene. \n",
    "You must strictly follow the format in the following examples and output exactly **one** action. \n",
    "The generated action must be strictly from **GROUNDED_ACTION_LIST**.\"\"\"\n",
    "        self.condition_list = ['ROOM_LIST', 'OBJECT_LIST', 'OBJECT_POSITION_LIST','CONTAINER_LIST', 'SURFACE_LIST', 'CONTAINER_POSITION_LIST', 'CONNECTED_ROOM','ACTION_DICT', 'GROUNDED_ACTION_LIST']\n",
    "        self.initial_conditions = initial_conditions\n",
    "        self.ROOM_LIST, self.OBJECT_LIST, self.OBJECT_POSITION_LIST, \\\n",
    "        self.CONTAINER_LIST, self.SURFACE_LIST, self.CONTAINER_POSITION_LIST, self.CONNECTED_ROOM, \\\n",
    "        self.ACTION_DICT, self.GROUNDED_ACTION_LIST = initial_conditions\n",
    "        \n",
    "        self.translation_lm = SentenceTransformer('all-MiniLM-L6-v2').to(self.device)\n",
    "        self.container_list_embedding = self.translation_lm.encode(self.CONTAINER_LIST, batch_size=8, \n",
    "                convert_to_tensor=True, device=self.device)  # lower batch_size if limited by GPU memory\n",
    "        self.object_list_embedding = self.translation_lm.encode(self.OBJECT_LIST, batch_size=8,\n",
    "                convert_to_tensor=True, device=self.device)\n",
    "        if self.SURFACE_LIST:\n",
    "            self.position_list =  self.CONTAINER_LIST + self.SURFACE_LIST\n",
    "            self.surface_list_embedding = self.translation_lm.encode(self.SURFACE_LIST, batch_size=8,\n",
    "                convert_to_tensor=True, device=self.device)\n",
    "            self.position_list_embedding = torch.concat((self.container_list_embedding, self.surface_list_embedding), dim=0)\n",
    "        self.room_embedding = self.translation_lm.encode(self.ROOM_LIST, batch_size=8,\n",
    "                convert_to_tensor=True, device=self.device)\n",
    "        self.action_list_embedding = self.translation_lm.encode(self.GROUNDED_ACTION_LIST, batch_size=8, \n",
    "                convert_to_tensor=True, device=self.device)  # lower batch_size if limited by GPU memory\n",
    "    \n",
    "    def find_most_similar(self, query_str, corpus_embedding):\n",
    "        # helper function for finding similar sentence in a corpus given a query\n",
    "        query_embedding = self.translation_lm.encode(query_str, convert_to_tensor=True, device=self.device, show_progress_bar=False,)\n",
    "        # calculate cosine similarity against each candidate sentence in the corpus\n",
    "        cos_scores = st_utils.pytorch_cos_sim(query_embedding, corpus_embedding)[0].detach().cpu().numpy()\n",
    "        # retrieve high-ranked index and similarity score\n",
    "        most_similar_idx = np.argmax(cos_scores)\n",
    "        return most_similar_idx\n",
    "    \n",
    "    def query_llm(self, task, observe = None):\n",
    "        prompt_content = self.prompt_begin\n",
    "        task = 'Scene: ' + \"\\n\".join(f\"{a}:{b}\" for a, b in zip(self.condition_list, self.initial_conditions) if a != 'ACTION_DICT') + '\\nCurrent State: ' + observe + 'Task: ' + task\n",
    "\n",
    "        \n",
    "        generated_samples = []\n",
    "        for _ in range(self.sampling_params['n']): # call llm for n times and append each response to 'generated_samples'\n",
    "            try: \n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        samples = generated_samples\n",
    "        # print(samples)\n",
    "        return samples\n",
    "    \n",
    "    def plan(self, task, observe):\n",
    "        samples = self.query_llm(task, observe = observe)\n",
    "        action_list, action_index = [], []\n",
    "        for sample in samples:\n",
    "            most_similar_idx = self.find_most_similar(sample, self.action_list_embedding)\n",
    "            # find the most similar action in the action list\n",
    "            translated_action = self.GROUNDED_ACTION_LIST[most_similar_idx]\n",
    "            action_list.append(translated_action)\n",
    "            action_index.append(most_similar_idx)\n",
    "        best_action = [max(action_list, key=action_list.count), max(action_index, key=action_list.count)]\n",
    "        # [action, index]\n",
    "        return best_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try On Your Own!\n",
    "\n",
    "You can try the given test environment to test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_q_learning_with_llm(Q_bool=False, index=0, num_episodes=1000, verbose=False, generate_solution=False):\n",
    "\n",
    "    instruction, goal_state, initial_conditions = test_cases[index]\n",
    "    \n",
    "    # ------- your code starts here ----- #\n",
    "\n",
    "    llm_model = LLM_Model(device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                          instruction=instruction,\n",
    "                          goal_state=goal_state,\n",
    "                          initial_conditions=initial_conditions, \n",
    "                          model = '')\n",
    "\n",
    "    # ------- your code ends here ------- #\n",
    "\n",
    "    \n",
    "    env = MiniHouseV1(\n",
    "        instruction=instruction,\n",
    "        goal_state=goal_state,\n",
    "        initial_conditions=initial_conditions,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    obs, reward, done, history, valid_action = env.reset()\n",
    "\n",
    "    best_action_llm = []\n",
    "    print('Your task is:' + instruction)\n",
    "    for state_index in range(env.nS):\n",
    "        print(\"state: \", state_index)\n",
    "        describe = describe_state(env.index_to_state(state_index))\n",
    "        best_action = llm_model.plan(instruction, observe=describe)\n",
    "        action_name = best_action[0]\n",
    "        action_index = best_action[1]\n",
    "        best_action_llm.append(action_index)\n",
    "        print('The best action is ' + action_name + '\\n')\n",
    "    \n",
    "    msq = mdp_solver_q_learning()\n",
    "    policy, V = msq.Q_learning(\n",
    "        alpha=0.1,\n",
    "        gamma=0.9,\n",
    "        theta=0.0001,\n",
    "        epsilon=0.1,\n",
    "        env_nS=env.nS,\n",
    "        env_nA=env.nA,\n",
    "        env_transition=env.transition,\n",
    "        env=env,\n",
    "        num_episodes=num_episodes,\n",
    "        initial_action = best_action_llm,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"state: \", env.state_to_index(env.state))\n",
    "        print(\"num state: \", env.nS)\n",
    "        print(\"num actions: \", env.nA)\n",
    "        print()\n",
    "\n",
    "    msq = mdp_solver_q_learning()\n",
    "\n",
    "    policy, V = msq.Q_learning(\n",
    "        alpha=0.1,\n",
    "        gamma=0.9,\n",
    "        theta=0.0001,\n",
    "        epsilon=0.1,\n",
    "        env_nS=env.nS,\n",
    "        env_nA=env.nA,\n",
    "        env_transition=env.transition,\n",
    "        env=env,\n",
    "        num_episodes=num_episodes,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Policy Iteration\")\n",
    "        print()\n",
    "        print(\"V: \", repr(V))\n",
    "        print()\n",
    "        print(\"policy: \", repr(policy))\n",
    "        print()\n",
    "        \n",
    "    env.reset()\n",
    "    \n",
    "    for i in range(100):\n",
    "        print()\n",
    "        print(f\"---------- Step: {i} ----------\")\n",
    "        action = int(policy[env.state_to_index(env.state)])\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            print(\"obs: \", obs)\n",
    "            print(\"reward: \", reward)\n",
    "            print(\"done: \", done)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if generate_solution:\n",
    "        np.savetxt(f'data/V_{index}.py', V, delimiter=',')\n",
    "\n",
    "        solution_values = np.loadtxt(f'data/V_{index}.py')\n",
    "\n",
    "        assert len(V) == len(solution_values), \\\n",
    "            'Length of Values is incorrect'\n",
    "\n",
    "        assert np.allclose(V, solution_values), \\\n",
    "            'Values incorrect'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q_learning_with_llm(index=0, num_episodes=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "Congratulations! You have successfully completed all the requirements for Problem 3.\n",
    "\n",
    "We hope that through this assignment, you have gained hands-on experience in leveraging LLMs for decision support in search problems and developed the following skills:\n",
    "\n",
    "1) Obtaining API keys and making calls to generative models.\n",
    "2) Crafting effective prompts to accurately describe the state and task.\n",
    "3) Interacting with the generative model and collecting meaningful responses.\n",
    "\n",
    "Following this, you should now have a deeper understanding of how to design effective prompts, process and validate model-generated actions. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
